
# Configuración del modelo
model: "llama3.2:3b"
temperature: 0.7
max_tokens: 2000        # Aumentado para modelos más capaces
top_p: 0.9
top_k: 40
context_size: 4096      # Aumentado para mejor contexto
# Configuración de la conversación
system_prompt: ""      # Ahora se maneja a través de archivos de personalidad
chat_history_limit: 20  # Aumentado para mejor contexto con modelos más capaces
save_conversations: true
conversation_dir: "conversations"

# Configuración de la aplicación
verbose: false
ollama_host: "http://localhost:11434"
api_key: "AAAAC3NzaC1lZDI1NTE5AAAAICXiy0gtY2IoK/1I0i5YV/stWE7U8+5tGaFXBG6IxuAw"
user_name: "Oz"  # Nombre del usuario para mostrar en el chat en lugar de "Tú"
use_panels: false  # Mostrar mensajes de Emma sin panel/recuadro

# Configuración de personalidades
personalities_dir: "personalities"  # Directorio donde se almacenan los archivos de personalidad